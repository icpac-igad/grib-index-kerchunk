# GEFS East Africa Ensemble Processing Pipeline

## Overview

Complete pipeline for processing GEFS (Global Ensemble Forecast System) ensemble data for the East Africa region using the **grib-index-kerchunk method** for efficient GRIB data access without full file scanning.

## 🔑 CRITICAL: The Two-Step Architecture

### Understanding the Grib-Index-Kerchunk Method

This pipeline uses a **two-step process** that separates expensive one-time preprocessing from fast daily processing:

#### **Step 0: One-Time Expensive Preprocessing** (ONLY ONCE PER ENSEMBLE MEMBER)
```bash
# Run ONCE to create reusable parquet mapping templates in GCS
python run_gefs_preprocessing.py  # Wrapper script for all members
# OR for individual member:
python gefs_index_preprocessing_fixed.py --date 20241112 --run 00 --member gep01 --bucket gik-gefs-aws-tf
```

**Purpose**: Create parquet mapping files that describe the GRIB data structure
- **When**: Run ONCE per ensemble member to create mapping templates
- **Creates**: Parquet files in GCS at `gs://bucket/gefs/{member}/gefs-time-{date}-{member}-rt{hour}.parquet`
- **Why expensive**: Scans actual GRIB files to build complete index mappings
- **⚡ Key benefit**: These parquet files can be reused across different dates!

#### **Daily Processing Workflow** (RUN FOR EACH NEW FORECAST)

```bash
# Step 1: Fast parquet creation using existing GCS mappings
python run_day_gefs_ensemble_full.py  # Uses mappings from Step 0

# Step 2: Convert all 30 ensemble members to zarr
for i in $(seq -f "%02g" 1 30); do
    python run_single_gefs_to_zarr_v2.py 20250709 00 gep$i \
        --region east_africa \
        --variables t2m,tp,u10,v10,cape,sp,mslet,pwat
done

# Step 3: Concatenate ensemble and compute statistics
python process_ensemble_by_variable.py zarr_stores/20250709_00/

# Step 4: Create plots (optional)
python run_gefs_24h_accumulation.py
python plot_ensemble_east_africa.py 
```

### How the Fast Daily Processing Works

1. **Uses existing parquet mappings** from GCS (created in Step 0)
2. **Reads only GRIB index** (.idx) files from new date's S3 data
3. **Combines** existing mapping structure with new index data
4. **Avoids expensive GRIB scanning** by reusing parquet mappings
5. **Result**: 10-100x faster processing for daily runs!

## Project Structure

```
gefs/
├── README.md                          # This file
│
├── 🔧 Core Components (One-Time Setup)
│   ├── gefs_index_preprocessing_fixed.py  # Step 0: Creates GCS parquet mappings
│   ├── run_gefs_preprocessing.py          # Step 0: Wrapper to process all members
│   └── gefs_util.py                       # Core utility functions
│
├── 📊 Daily Processing Scripts
│   ├── run_day_gefs_ensemble_full.py      # Step 1: Fast parquet creation
│   ├── run_single_gefs_to_zarr_v2.py      # Step 2: Single member to zarr
│   ├── process_ensemble_by_variable.py    # Step 3: Ensemble concatenation
│   └── run_gefs_24h_accumulation.py       # Step 4: Plotting routines
│
├── 📁 Configuration & Data Files
│   ├── vz_gik_env.yaml               # Conda environment specification
│   └── ea_ghcf_simple.geojson        # East Africa region boundary
│
├── 📚 Documentation
│   ├── docs/
│   │   ├── GEFS_Complete_Documentation.md # Complete technical documentation
│   │   ├── GEFS_PWAT_Skipping_Analysis.md # PWAT variable troubleshooting
│   │   └── windows-setup.md              # Windows environment setup
│   └── GEFS_PWAT_Skipping_Analysis.md    # Root-level PWAT analysis
│
├── 🧪 Testing & Development Scripts
│   ├── test_compare_dynamical_zarr_gefs_24h_accumulation.py
│   ├── create_gefs_east_africa_netcdf.py
│   └── plot_ensemble_east_africa.py
│
└── 📂 Output Directories (created automatically)
    ├── YYYYMMDD_HH/                  # Parquet files from Step 1
    │   ├── gep01.par
    │   ├── gep02.par
    │   └── ...
    └── zarr_stores/                   # Zarr stores from Step 2
        └── YYYYMMDD_HH/
            ├── gep01_combined_east_africa.zarr
            ├── gep02_combined_east_africa.zarr
            └── ...
```


## Processing Pipeline Details

### Step 0: One-Time GCS Mapping Creation (`gefs_index_preprocessing_fixed.py`)

**⚠️ IMPORTANT**: This step is run ONCE and creates reusable mappings stored in Google Cloud Storage

**Purpose**: Create comprehensive GRIB structure mappings that can be reused across dates

```bash
# Process all 30 members (recommended)
python run_gefs_preprocessing.py

# OR process individual member
python gefs_index_preprocessing_fixed.py \
    --date 20241112 \
    --run 00 \
    --member gep01 \
    --bucket gik-gefs-aws-tf
```

**Input**:
- GEFS GRIB files from NOAA S3 (full file scanning)

**Output**:
- Parquet mappings in GCS: `gs://bucket/gefs/{member}/gefs-time-{date}-{member}-rt{hour}.parquet`
- One parquet file per forecast hour (000, 003, 006, ..., 240)

**Frequency**:
- **ONE TIME ONLY** per ensemble member
- Mappings are reusable across different forecast dates
- Only re-run if GRIB structure changes (rare)

### Step 1: Fast Daily Parquet Creation (`run_day_gefs_ensemble_full.py`)

**Purpose**: Creates local parquet files using GCS mappings + fresh GRIB indices
- `TARGET_DATE_STR = '20250709'`  # Date to process
- `TARGET_RUN = '00'`              # Run hour (00, 06, 12, 18)
- `ENSEMBLE_MEMBERS = [f'gep{i:02d}' for i in range(1, 31)]`  # All 30 members
- `KEEP_PARQUET_FILES = True`      # Keep intermediate parquet files

**Input**:
- GEFS GRIB files from NOAA S3: `s3://noaa-gefs-pds/gefs.YYYYMMDD/HH/atmos/pgrb2sp25/`

**Output**:
- Parquet files: `YYYYMMDD_HH/gep01.par`, `YYYYMMDD_HH/gep02.par`, etc.
- These files contain zarr reference mappings for efficient data access

**Note**: These parquet files can be reused across different dates with the same forecast structure

### Step 2: Individual Member Processing (`run_single_gefs_to_zarr_v2.py`)

**Purpose**: Convert parquet reference files to actual zarr stores with regional subsetting

```bash
# Process single member
python run_single_gefs_to_zarr_v2.py 20250709 00 gep01 \
    --region east_africa \
    --variables t2m,tp,u10,v10,cape,sp,mslet,pwat

# Process all 30 members (bash loop)
for i in $(seq -f "%02g" 1 30); do
    python run_single_gefs_to_zarr_v2.py 20250709 00 gep$i \
        --region east_africa \
        --variables t2m,tp,u10,v10,cape,sp,mslet,pwat
done
```

**Arguments**:
- `date`: Date in YYYYMMDD format (e.g., 20250709)
- `run`: Run hour in HH format (00, 06, 12, 18)
- `member`: Ensemble member (gep01-gep30)
- `--region`: Region to subset (global, east_africa)
- `--variables`: Comma-separated list of variables (optional, default: all)
- `--parquet_dir`: Directory with parquet files (default: YYYYMMDD_HH)

**Features**:
- **98.2% size reduction** for East Africa region
- Single unified zarr per ensemble member
- Variable filtering capabilities
- Enhanced compression using zstd
- CF-compliant metadata

**Output**:
- Zarr stores: `zarr_stores/YYYYMMDD_HH/gep01_combined_east_africa.zarr`

### Step 3: Ensemble Concatenation (`process_ensemble_by_variable.py`)

**Purpose**: Create ensemble NetCDF files and compute statistics

```bash
# Process all variables
python process_ensemble_by_variable.py zarr_stores/20250709_00/

# Process specific variables only
python process_ensemble_by_variable.py zarr_stores/20250709_00/ --variables t2m,tp,cape

# Skip statistics computation
python process_ensemble_by_variable.py zarr_stores/20250709_00/ --no-stats

# Custom output directory
python process_ensemble_by_variable.py zarr_stores/20250709_00/ --output ensemble_output/
```

**Arguments**:
- `zarr_dir`: Directory containing zarr stores from all ensemble members
- `--variables`: Comma-separated list of variables (optional)
- `--no-stats`: Skip statistics computation
- `--output`: Output directory for NetCDF files
- `--verbose`: Enable verbose logging

**Processing Strategy**: Variable-by-variable for memory efficiency
1. Load one variable from all 30 members
2. Concatenate along new 'member' dimension
3. Save as NetCDF
4. Compute statistics (mean, std)
5. Clear memory and move to next variable

**Output**:
- `ensemble_{variable}.nc`: All 30 members concatenated
- `ensemble_mean_{variable}.nc`: Ensemble mean
- `ensemble_std_{variable}.nc`: Ensemble standard deviation

### Step 4: Plotting and Analysis (`run_gefs_24h_accumulation.py`)

**Purpose**: Create visualization plots and accumulation analysis

```bash
python run_gefs_24h_accumulation.py
python plot_ensemble_east_africa.py 
```

**Features**:
- 24-hour precipitation accumulation
- Ensemble mean and spread plots
- East Africa regional focus
- Empirical probability calculations

**Output**:
- PNG plots in `plots/` directory
- Analysis reports

## Available Variables

### Primary Variables (with PWAT fix applied)

| Variable | Description | Units | Dimensions |
|----------|-------------|-------|------------|
| `t2m` | 2-meter temperature | K | (time, lat, lon) |
| `tp` | Total precipitation | mm | (time, lat, lon) |
| `u10` | 10m U wind component | m/s | (time, lat, lon) |
| `v10` | 10m V wind component | m/s | (time, lat, lon) |
| `cape` | Convective available potential energy | J/kg | (time, lat, lon) |
| `sp` | Surface pressure | Pa | (time, lat, lon) |
| `mslet` | Mean sea level pressure | Pa | (time, lat, lon) |
| `pwat` | Precipitable water | kg/m² | (time, lat, lon) |

### Variable Mapping

| GRIB Name | Zarr Name | Description |
|-----------|-----------|-------------|
| `PRES:surface` | `sp` | Surface pressure |
| `TMP:2 m above ground` | `t2m` | 2-meter temperature |
| `UGRD:10 m above ground` | `u10` | 10m U wind |
| `VGRD:10 m above ground` | `v10` | 10m V wind |
| `PWAT:entire atmosphere (considered as a single layer)` | `pwat` | Precipitable water |
| `CAPE:surface` | `cape` | CAPE |
| `MSLET:mean sea level` | `mslet` | Mean sea level pressure |
| `APCP:surface` | `tp` | Total precipitation |

## Environment Setup

### Using Conda/Mamba

```bash
# Create environment from specification
mamba env create -f vz_gik_env.yaml

# Activate environment
conda activate gefs-processing
```

### Using Coiled (Cloud Computing)

```bash
# Start Coiled notebook
coiled notebook start \
    --name dask-gefs \
    --vm-type n2-standard-2 \
    --software gik-zarr2 \
    --workspace=geosfm

# Upload required files to Coiled:
# - gefs_util.py
# - run_day_gefs_ensemble_full.py
# - run_single_gefs_to_zarr_v2.py
# - process_ensemble_by_variable.py
# - run_gefs_24h_accumulation.py
# - ea_ghcf_simple.geojson
# - Service account JSON (if using GCS)
```

## Memory and Performance Considerations

### Memory Requirements
- **Step 1**: ~4GB RAM (parquet creation)
- **Step 2**: ~2GB RAM per ensemble member
- **Step 3**: ~8GB RAM (variable-by-variable processing)
- **Step 4**: ~4GB RAM (plotting)

### Performance Tips
1. **Parallel Processing**: Steps 2 can be parallelized across ensemble members
2. **Disk Space**: ~500MB per ensemble member for East Africa region
3. **Network**: Requires stable internet for S3 access
4. **Compression**: Zarr stores use zstd compression for optimal size/speed

## Troubleshooting

### Common Issues

1. **PWAT Variable Missing**
   - Solution: Ensure gefs_util.py has the regex fix applied
   - See `docs/GEFS_PWAT_Skipping_Analysis.md` for details

2. **Memory Errors**
   - Use `--variables` flag to process subset of variables
   - Process ensemble members in smaller batches
   - Increase system swap space

3. **S3 Access Errors**
   - Ensure `AWS_NO_SIGN_REQUEST=YES` is set for anonymous access
   - Check internet connectivity
   - Retry with exponential backoff for transient errors

4. **Zarr Version Compatibility**
   - Requires zarr < 3.0 for datatree support
   - Check with: `python -c "import zarr; print(zarr.__version__)"`

### Debug Mode

Enable verbose logging for detailed diagnostics:
```bash
# Add --verbose flag to any script
python run_single_gefs_to_zarr_v2.py 20250709 00 gep01 --verbose
python process_ensemble_by_variable.py zarr_stores/20250709_00/ --verbose
```

## Data Access Patterns

### Reading Output Files

```python
import xarray as xr

# Read single ensemble member
ds_member = xr.open_zarr('zarr_stores/20250709_00/gep01_combined_east_africa.zarr')

# Read ensemble concatenated file
ds_ensemble = xr.open_dataset('ensemble_output/ensemble_t2m.nc')

# Read ensemble statistics
ds_mean = xr.open_dataset('ensemble_output/ensemble_mean_t2m.nc')
ds_std = xr.open_dataset('ensemble_output/ensemble_std_t2m.nc')
```

## File Naming Conventions & Suggestions

### Current Naming (Good)
- ✅ `gefs_index_preprocessing_fixed.py` - Clear purpose (preprocessing)
- ✅ `run_day_gefs_ensemble_full.py` - Indicates daily run
- ✅ `process_ensemble_by_variable.py` - Descriptive action
- ✅ `gefs_util.py` - Standard utility module name

### Suggested Improvements for Clarity

Consider renaming for better understanding of the workflow:

```bash
# One-time setup scripts (Step 0)
gefs_index_preprocessing_fixed.py  → gefs_00_create_gcs_mappings.py
run_gefs_preprocessing.py          → gefs_00_create_all_mappings.py

# Daily processing scripts (Steps 1-4)
run_day_gefs_ensemble_full.py      → gefs_01_daily_parquet_creation.py
run_single_gefs_to_zarr_v2.py      → gefs_02_member_to_zarr.py
process_ensemble_by_variable.py    → gefs_03_ensemble_concatenate.py
run_gefs_24h_accumulation.py       → gefs_04_plot_analysis.py
```

This naming scheme:
- Numbers indicate processing order
- Clear separation between one-time (00) and daily (01-04) scripts
- Self-documenting file names

## References

- [NOAA GEFS on AWS](https://registry.opendata.aws/noaa-gefs/)
- [Kerchunk Documentation](https://fsspec.github.io/kerchunk/)
- [Zarr Documentation](https://zarr.readthedocs.io/)
- Complete technical details: `docs/GEFS_Complete_Documentation.md`

## Acknowledgements

This work was funded in part by:

1. Hazard modeling, impact estimation, climate storylines for event catalogue
   on drought and flood disasters in the Eastern Africa (E4DRR) project.
   https://icpac-igad.github.io/e4drr/ United Nations | Complex Risk Analytics
   Fund (CRAF'd) 
2. The Strengthening Early Warning Systems for Anticipatory Action (SEWAA)
   Project. https://cgan.icpac.net/

